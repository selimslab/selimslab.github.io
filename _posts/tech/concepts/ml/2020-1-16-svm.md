---
layout: post
title: Support vector machines 
tags: machine-learning 
category: tech/concepts

--- 

Built upon Simple Intuitive maximal margin classifier 

MMC requires classes to be separable by a linear boundary

SVC solves a broader case 

SVC uses a soft margin to solve inseparable cases 

a few samples is on the incorrect side, but the general public is better

SVM is a further extension and can solve nonlinear boundaries

SVM is able to create nonlinear boundaries

One way is enlarging the feature space 

A better way is using a kernel 

A kernel is simply a function 

Instead of the inner product of features, we apply a function to it, and call this function a kernel 
 
This essentially means fitting an SVC in a higher dimensional space 

Using a kernel is computationally efficient


SVM is connected to logistic regression, their loss functions are very similar

Also kernels can be used with other methods, too. Although they are used with SVMs mostly 

You can use SVMs for more than 2 classes. 

+ one vs one
  Create SVMs for all 2-pairs of classes. In the end a sample belongs to the class to which it is most frequently assigned 
  
+ one vs all