---
layout: post
title: Notes on machine learning concepts
--- 

# Basics

Standard deviation = How far every sample from the mean on average? 

variance is the square of std

RSS, residual sum of squares = sum of differences between true values and estimates 

RSE, residual standard error = how far away the estimates from true values on average 

R2 is between 0 and 1, near 1 means the model explains most of the variability 

#### t statistic

How many std deviations is the value away from 0 

+ Ïƒ 68
+ 2Ïƒ 95
+ 3Ïƒ 99


For example, 2Ïƒ is the 95% confidence interval 

#### Z value

For example z=2 covers 95% of the distribution

#### P value 

Probability of getting this result by chance rather than the effect of the predictor 


Measure on the test set, good metrics on training set has little meaning 

<br>


Supervised learning is predicting or estimating an output based on some input 

+ 1800s least squares and linear regression
+ 1936 LDA
+ 40s logistic regression
+ 70s generalized linear models 
+ 80s classification and regression trees
+ 86 generalized additive models


<br>


# Linear regression 

+ is at least one predictor related? 
+ Hypothesis test
+ F statistic


##### Which variables are important?

1. Forward selection 
Start with nothing and add one by one with lowest RSS

2. Backward 
Start with all and drop 1 by 1 by highest p value 

Mixed 

How well the model fits? Look at RSE and R2


### Prediction 

Inference is how features effect the outcome 

Reducible and irreducible error due to epsilon 

Y = f(X) + e

How to estimate f?

1. Parametric, high bias 

2. Non parametric, requires a lot of input 


More flexible models are less interpretable

And they are not always more accurate, due to overfitting 

Interpretability is good for inference or understanding the effect of a single parameter 

Mean square error 

degrees of freedom

Cross validation 

#### Bias & Variance

Less flexible methods have more bias 

In general more flexible methods has more variance 

Good test performance requires low bias and low variance 

MSE consists of bias variance and other error terms 


### Classification 

+ Training error
+ Test error

Smallest test error = better classifier 

Minimum test error rate is Bayes error rate and the resulting boundary is the Bayes decision boundary. This is analogous to the irreducible error 

Bayes is gold standard but mostly impossible because we don't know the conditional distribution Y given X 

Many approaches like knn attempt to estimate the conditional probability and use it for Bayes 


##### one vs all
is this class1? is this class2? and so on 

##### Binary Relevance
Ensemble of single-label binary classifiers is trained, one for each class.

Transform the multi-label problem into a set of binary classification problems, or adapt the algorithms


<br>


## Linear model selection and regularization

Linear models has an advantage of interpretability 

Most basic ones are fitted by least squares

How they can be improved ? 

#### Subset selection 

Selecting a specific sunset of predictors, also known as variable selection or feature selection 

#### Shrinkage or regularization

a model is fit using all p predictors but coefficients of some are reduced toward 0 

Ridge regression makes coefficients small

Lasso makes them 0 

#### dimension reduction 
projecting predictors to a lower dimensional space 

#### PCA principal components analysis
find the directions if data along which the observations vary the most 

#### PCR, use principal components as regression variables

Unsupervised, because there is no guarantee that directions best explain the pc will be useful to predict the response 

#### Partial least squares 
Supervised alternative to PCR 


## Resampling
+ Cross validation 
+ K-fold CV, high bias 
+ Leave one out CV,  high variance 
+ Bootstrap is Resampling with replacement 


## Logistic regression
Models the probability of belonging to a specific class 

Between 0 and 1 naturally


## Linear discriminant analysis

Assume a Gaussian distribution

Mean is the mean of samples

Variance is calculated by samples 

Assumes each class has the same covariance matrix 

Get the linear decision boundaries between Gaussians

## Comparison of LDA and logistic regression
Both produce linear decision boundaries 

Logistic parameters are estimated by maximum likelihood, while LDA parameters are computed using the estimated mean and variance from a normal distribution

Both create very similar boundaries. When Gaussian assumption is off, logistic performs better 

+ Knn is nonparametric, highly flexible 

## QDA 
Assumes each class has its own covariance matrix 

QDA is the middle ground between LDA and knn 



# Beyond linearity 

Linear models have the advantage of interpretability and inference 

But not suitable for everything
They can be extended by

Polynomial regression 

Step functions
Fit piecewise functions on distinct regions

Regression splines 
Fit smoothly joining polynomials on distinct regions

Generalized additive models, allows to do the above with multiple predictors 

Adding a multiplier term p1.p2 could help to explain interaction between variables, 




# Trees ðŸŒ²

## Regression tree

Recursive binary splitting 

Consider all predictors and all possible values of cutpoint, choose the one resulting with the lowest RSS tree

Tree pruning 

Cost complexity pruning 

## Classification trees

Make recursive splits by classification error rate, Gini index, or cross entropy

Categorical variables are ok, no need for dummy variables

trees are easy to explain, display, interpret

<br>

Tree prediction performance can be improved by bagging, boosting, aggregating many decision trees


## Bagging

Trees have high variance and bagging is way to reduce the variance 

Bootstrap aggregation 

create many sets from the same data using sampling with replacement 

Create many trees and aggregate them 

To estimate test error, no need for CV or validation set 
instead use
+ out of the bag observations
+ out of the bag error

Bagging improves predicton accuracy but decreases interpretability

Important predictors can be seen by averaging RSS for regression trees or Gini index for classification trees


## Boosting

Just as we learn from previous mistakes, models can learn from previous models

+ Sequential training 

+ Adaboost gives more weight to incorrectly classified samples next time

+ gradient boosting 

+ xg boost , extreme gradient boosting
Since boosting is sequential, it's slow. Xgboost parallelizes it 



## Random forests ðŸŒ³ðŸŒ´ðŸŒ²

Select a random sample of m predictors 

Thus, decorrelate the trees

As a measure of node purity, Gini index and cross entropy could be used.


## Support vector machines 

Built upon Simple Intuitive maximal margin classifier 

MMC requires classes to be separable by a linear boundary

SVC solves a broader case 

SVC uses a soft margin to solve inseparable cases 

a few samples is on the incorrect side, but the general public is better

SVM is a further extension and can solve nonlinear boundaries

SVM is able to create nonlinear boundaries

One way is enlarging the feature space 

A better way is using a kernel 

A kernel is simply a function 

Instead of the inner product of features, we apply a function to it, and call this function a kernel 
 
This essentially means fitting an SVC in a higher dimensional space 

Using a kernel is computationally efficient


SVM is connected to logistic regression, their loss functions are very similar

Also kernels can be used with other methods, too. Although they are used with SVMs mostly 

To use SVMs for more than 2 classes

1. one vs one 

Create SVMs for all 2-pairs of classes, 

In the end a sample belongs to the class to which it is most frequently assigned 

2. one vs all


## PCA

variables should be scaled and centered to have mean 0 

The vectors are unique, different software will produce the same results 

> ROC curve is TP vs FP



## Speech recognition

https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a

## CNNs

https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721

## Face Recognition 

Histogram of Oriented Gradients(HOG)

1. picture to grayscale
2. pixels to gradients
3. Downsample gradients, 4x4 boxes to 1 gradient
4. Deal with posing using 68 landmarks and  basic image transformations like rotate, scale, shear
5. Gradients to a matrix of numbers using triplet training
6. Compare matrices

